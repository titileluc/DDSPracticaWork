---
title: "02 - PreProcessing using Random Forest"
author: "Sergi Puigventos Ventura & Daniel Da Gra√ßa"
date: "`r Sys.Date()`"
output: html_document
---
```{r setup, include=FALSE, results='hide'}
knitr::opts_chunk$set(echo = TRUE)

# Packages required for the project
required_packages <- c('rstudioapi', 'fs', 'stringr', 'knitr', 'randomForest', 'readr', 'caret', 'e1071', 'tidyverse'
                       , 'ggplot2', 'dplyr', 'corrplot', 'psych', 'vcd', 'kableExtra')

# Installation of all required packages not already installed
install.packages(setdiff(required_packages, rownames(installed.packages())))

# Loading required packages
library('rstudioapi', warn.conflicts = FALSE)
library('fs', warn.conflicts = FALSE)
library('stringr', warn.conflicts = FALSE)
library('randomForest', warn.conflicts = FALSE)
library('readr', warn.conflicts = FALSE)
library('caret', warn.conflicts = FALSE)
library('e1071', warn.conflicts = FALSE)
library('tidyverse', warn.conflicts = FALSE)
library('ggplot2', warn.conflicts = FALSE)
library('dplyr', warn.conflicts = FALSE)
library('corrplot', warn.conflicts = FALSE)
library('psych', warn.conflicts = FALSE)
library('vcd', warn.conflicts = FALSE)
library('kableExtra', warn.conflicts = FALSE)

# Get this rmd file current absolute path 
# (from: https://stackoverflow.com/questions/47044068/get-the-path-of-current-script)
if (rstudioapi::isAvailable()) {
    wdir <- dirname(getActiveDocumentContext()$path)
}else{
    wdir <- getwd()
}
tryCatch(expr = {setwd(wdir)}, 
         error = function(e){print(e)}, 
         warning = function(w){print(w)})
```

## Introducci√≥n
<br>

En esta secci√≥n probamos formas de mejorar el modelo provisto para la pr√°ctica basado en el algoritmo de Random Forest.

<br>

Preguntando a chatGPT en que consiste el algoritmo de Random Forest, √©l explica que ese algoritmo es "<i>un m√©todo de aprendizaje conjunto que combina varios √°rboles de decisi√≥n para realizar predicciones. Es un algoritmo de aprendizaje supervisado que se utiliza tanto para tareas de regresi√≥n como de clasificaci√≥n.</i>"

En nuestro caso, el objetivo siendo de predecir los ataques, se utilizar√° su funci√≥n de regresi√≥n. Para eso, el algoritmo "<i>construye √°rboles de decisi√≥n utilizando los datos de entrenamiento y las caracter√≠sticas seleccionadas. Los √°rboles se construyen mediante un proceso recursivo denominado partici√≥n recursiva, que divide los datos en subconjuntos en funci√≥n de los valores de las caracter√≠sticas. Una vez construidos todos los √°rboles, cada uno de ellos realiza una predicci√≥n. Se utiliza como predicci√≥n final la media o la mayor√≠a de los votos de las predicciones de los √°rboles.</i>"

ChatGPT tambi√©n a√±ade que "<i>las principales caracter√≠sticas del algoritmo Random Forest incluyen su capacidad para manejar datos de alta dimensi√≥n, manejar valores perdidos y valores at√≠picos, y proporcionar estimaciones de la importancia de las caracter√≠sticas</i>", lo que conviene bien para este ejercicio dado que muchas de las caracter√≠sticas del fichero tienen una distribuci√≥n poco normal, distribuciones concentradas en uno o muy pocos valores, o con muchos outliers (de hecho en la fase de exploraci√≥n dejamos de utilizar boxplots para describir los datos porque no permit√≠an ilustrar nada).

ChatGPT concluye que "<i>en general, el algoritmo Random Forest es conocido por su versatilidad, precisi√≥n y capacidad para manejar conjuntos de datos complejos, lo que lo convierte en una opci√≥n popular para diversas tareas de aprendizaje autom√°tico</i>", lo que para nosotros ne√≥fitas no est√° de m√°s. üòä  

<br>

La forma que hemos entonces seguido para intentar mejorar el modelo ha sido de ejecutar varios intentos cambiado en cada uno algunas ideas que se nos ocurrieron desde que empezamos a trabajar en esta pr√°ctica.
<br>
<br>

1. Lo primero obviamente ha sido de testar el modelo original (punto de partida de la pr√°ctica) realizando varias iteraciones cambiando el par√°metro especificando la cantidad de √°rboles en el randomforrest() de forma a llegar el valor que creemos que desempe√±aba mejor. 

2. El primero intento consisti√≥ en simplemente substituir las variables provistas en el modelo original por las que se hab√≠an identificado como teniendo m√°s correlaci√≥n con la respuesta "Attack". 

3. El segundo intento trat√≥ de substituir las variables provistas en el modelo original por
   + las que se han identificado como teniendo la mayor correlaci√≥n (coefficient >= 0.7 & p-value <= 0.05) con cada una de las variables respuestas que derivamos de los valores de "Attack" en la <a href="./01---Initial-Data-Discovery.html">fase inicial de exploraci√≥n de los datos</a>
   + pero tambi√©n aquellas que no estaban entre las m√°s fuertes correlaciones pero aparec√≠an entre las pocas identificadas para aquellas variables respuestas con pocas correlaciones. 

4. El tercero intento fue poco diferente del anterior. En este solo 
   + substituimos las variables provistas en el modelo original por las que se han identificado como teniendo la mayor correlaci√≥n con las variables respuestas derivadas desde "Attack"  
   + y con las que tienen buena relaci√≥n con la variable gen√©rica "IsAttack". 

5. Con el cuarto intento, substituimos las variables provistas en el modelo original por  
   + las que se han identificado como teniendo una correlaci√≥n con las variables respuestas derivadas desde "Attack" con un coeficiente >= 0.6,
   + manteniendo las del intento 02,
   + y DstHostRerrorRate del intento 03, pero removiendo RerrorRate, SrvRerrorRate, DstHostSrvRerrorRate porque tienen una correlaci√≥n entre ellas muy fuerte (coef. > 0.96) lo que puede acabar siendo redundante y al final no ayudar, entonces nos quedamos con aquella de las cuatro que tiene mayor coeficiente de correlaci√≥n con la variable respuesta "Attack". 

6. Para el quinto intento decidimos  
   + seguir con las variables utilizadas en el cuarto intento  
   + pero a√±adi√©ndoles una nueva, TotalBytes = SrcBytes + DstBytes. 

7. En el sexto intento  
   + conservamos las variables del quinto intento  
   + pero esta vez a√±adi√©ndoles las variables que hab√≠an quedado fuera hasta ahora, pero tienen un coeficiente de correlaci√≥n >= 0.5 con las variables respuestas. 

8. Con el s√©ptimo intento  
   + conservamos las mismas variables que en el quinto intento  
   + pero, para intentar predecir mejor los tipos de ataques U2R, siendo este el tipo m√°s grave que deber√≠amos identificar, se les a√±adi√≥ una ‚Äúfeature‚Äù nueva en base a una combinaci√≥n de aquellas que tienen mayor correlaci√≥n con este tipo de ataques, NumCompromised, NumRoot, NumShells y NumFile. 

9. Para el octavo intento, 
   + conservamos las mismas variables que en el quinto intento  
   + pero substituyendo algunas variables num√©ricas por otra que creemos que las resumen. 

<br>

Nuestro mejor modelo habr√° sido el octavo. Acert√≥ bastante bien con solo 1.15 % de errores contra el fichero de testing pero contra el fichero data_full no fue igual ya que observamos 6.96% de errores.

```{r Final_Results_InIntro, echo=FALSE, cache=TRUE}

cat(paste("         -------------------------------------\n         |    Modelo 8    | Contra data_full |\n"
          ,"--------|----------------|------------------|\n | FALSE |   800 |  1.15% |   21656 |  6.96% |\n"
          , "|  TRUE | 68742 | 98.85% |  289373 | 93.04% |\n --------|----------------|------------------|"), fill = TRUE)
```


Por falta de m√°s tiempo ya lo dejamos aqu√≠ pero entendemos que habr√≠a mucho m√°s que mirar para intentar mejorar esta predicci√≥n, como por ejemplo: 
   * intentar utilizar algoritmos para clasificar las caracter√≠sticas y as√≠ identificar padrones que se nos habr√°n escapado,
   * intentar jugar con el tama√±o de los ficheros de training y testing para optimizar los modelos derivados,
   * intentar encontrar modelos para categor√≠a de ataque de forma a verificar si especializando los modelos y descartando los ataques ya encontrados se podr√≠a mejorar todav√≠a m√°s el desempe√±o,
   * conseguir utilizar otros algoritmos predictivos para compararlos con los resultados del Random Forest (intentamos sin √©xito usar el modelo neuronal - fit(), SVM -  svm() o CART - train()). 
<br><br>


Cosas que se dejaron de hacer (por falta de tiempo):

   * intentar utilizar algoritmos para clasificar las caracter√≠sticas y as√≠ identificar padrones que se nos habr√°n escapado;
   * intentar encontrar modelos para categor√≠a de ataque de forma a verificar si especializando los modelos y descartando los ataques ya encontrados se podr√≠a mejorar todav√≠a m√°s el desempe√±o;
   * conseguir utilizar otros algoritmos predictivos para compararlos con los resultados del Random Forest (intentamos sin √©xito usar el modelo neuronal - fit(), SVM -  svm() o CART - train()). 
<br><br>

Al final esto todo es para decir que con m√°s tiempo hay mucho m√°s por explorar que podr√≠a talvez servir mejor el objetivo de predicci√≥n de ataques. 
<br><br>

```{r read_data, include=FALSE, echo=FALSE}
readingdataOK <- TRUE
tryCatch(
    expr = {
        # Loading the reference data set with the conclusions from the Initial Data Exploration stage
        features <- read.csv (file="features(2).csv",header=T)

        # Loading the full dataset
        data_full <- read_csv("Book1.csv",
                          col_types = cols(SrcBytes = col_integer(),
                                           DstBytes = col_integer(), Land = col_integer(),
                                           WrongFragment = col_integer(), Urgent = col_number(),
                                           Hot = col_number(), NumFailedLogin = col_integer()))
        
        # Loading the training dataset
        data <- read.csv (file="Book2.csv",header=T)
        nrowsbeforecleaning <- nrow(data)        
        nrespvars <- 1

        # Removing eventual duplicated rows 
        data <- unique(data)
        nrows <- nrow(data)
        if(nrows != nrowsbeforecleaning){
            cat(paste('Number of duplicated rows removed:', nrowsbeforecleaning - nrows), fill = TRUE)
        }

        # Cleaning the Response variable which for some reason contains a dot (".") which we preferred to remove
        data$Attack <- gsub("[.,]", "", data$Attack)
                
        # Loading a data set with a categorization of the response variable: attack types 
        # (https://kdd.org/cupfiles/KDDCupData/1999/training_attack_types)        
        attackcats <- read.csv (file="attack_types.csv",header=F, col.names = c("Attack", "Category"))
        
        # Addition of response variable features based on the data$Attack values
        data$AttackCat <- apply(data, 1, function(row){
            category <- attackcats$Category[attackcats$Attack == row['Attack']]
            if(row['Attack']=='normal'){
                category <- 'normal'
            }else if (length(category) == 0) {
                category <- "other"
            }else if (length(category) > 1) {
                category <- category[1]
            }
            return(category)
        })
        nrespvars <- nrespvars + 1
        
        data$IsAttack <- apply(data, 1, function(row){
            isattack <- 0
            if(row['Attack']!='normal'){
                isattack <- 1
            }
            return(isattack)
        })
        nrespvars <- nrespvars + 1

        data$IsDosAttack <- apply(data, 1, function(row){
            isattack <- 0
            if(row['AttackCat']!='dos'){
                isattack <- 1
            }
            return(isattack)
        })
        nrespvars <- nrespvars + 1

        data$IsProbeAttack <- apply(data, 1, function(row){
            isattack <- 0
            if(row['AttackCat']!='probe'){
                isattack <- 1
            }
            return(isattack)
        })
        nrespvars <- nrespvars + 1
        
        data$IsR2Lattack <- apply(data, 1, function(row){
            isattack <- 0
            if(row['AttackCat']!='r2l'){
                isattack <- 1
            }
            return(isattack)
        })
        nrespvars <- nrespvars + 1
        
        data$IsU2Rattack <- apply(data, 1, function(row){
            isattack <- 0
            if(row['AttackCat']!='u2r'){
                isattack <- 1
            }
            return(isattack)
        })
        nrespvars <- nrespvars + 1
        
        data$IsOtherAttack <- apply(data, 1, function(row){
            isattack <- 0
            if(row['AttackCat']!='other'){
                isattack <- 1
            }
            return(isattack)
        })
        nrespvars <- nrespvars + 1
        
        nfeatures <- nrow(features)
        ncols <- ncol(data)
        if(ncols != nfeatures || ncols == 0 || nrows == 0){
            if(ncols != nfeatures){
                print(paste("The number of cols on the trainig dataset,", ncols, 
                          "does not match the expected number of features", nfeatures, "."), row.names = FALSE)
            }else{
                print('There may be some issue with the dataset.', row.names = FALSE)
            }
            readingdataOK <- FALSE
        }

    }, error = function(e){
        readingdataOK <- FALSE
        print(e)
    }, warning = function(w){
        print(w)
    }
)


```



## Consideraciones por "feature" sacadas de la <a href="./01---Initial-Data-Discovery.html">fase inicial de exploraci√≥n de los datos</a>
<br><br>

```{r features_considerations, echo=FALSE, cache=TRUE}
# Create the table using kable
kableExtra::kable(features[,c('Feature', 'StatType', 'Description', 'Keep', 'Observations')]
                  , booktabs = TRUE, escape = FALSE) %>% kable_styling(full_width = TRUE)
```
<br>



### Ejecuci√≥n del modelo original 
<br>

Esta ejecuci√≥n del modelo original (punto de partida de la pr√°ctica) nos va servir de marco para determinar si hay mejora de performance del modelo con los dem√°s intentos. 
Adem√°s tambi√©n nos ha permitido encontrar el valor del par√°metro especificando la cantidad de √°rboles en el randomforrest() con el cual creemos que el modelo original desempe√±a mejor. Despu√©s de varias iteraciones, aunque los resultados raramente son exactamente los mismos, llegamos a la conclusi√≥n que el valor 100 es el que creemos que permite al modelo desempe√±ar mejor. 
Al final compararemos los dem√°s modelos con las estad√≠sticas b√°sicas de acierto, siendo estas en el caso del modelo original: 
   + FALSE:     1835, 
   + TRUE:      67707.

```{r intent_00_feature_selection, cache=TRUE}
data1 <- data[,c("SrcBytes", "DstBytes", "Land", "WrongFragment", "Urgent", "SameSrvRate", "LoggedIn",  "DstHostSameSrvRate", "DstHostSrvCount","Flag","Attack" )]
data1$Attack <- as.factor(data1$Attack)
```
```{r intent_00_train_test, echo=FALSE, cache=TRUE}
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_00_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 100)
plot(output.forest, main="Modelo Inicial provisto para la pr√°ctica")
```
```{r intent_00_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_00_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 01
<br>
El primero intento consisti√≥ en simplemente substituir las variables provistas en el modelo original por las que se hab√≠an identificado como teniendo correlaci√≥n con la variable respuesta "Attack". 

Desafortundamente este primero intento acab√≥ dando unos resultados mucho peores que los del modelo original con FALSE: 5691, TRUE: 63851. 

De este experimento, fuera considerar que talvez nos quedar√≠amos con las variables que ten√≠an una correlaci√≥n fuerte, no sacamos nada.  

```{r getting_Attack_correlations, cache=TRUE, echo=FALSE}
tryCatch(
    expr = {knitr::kable(corrs[(corrs$feature1 == 'Attack' | corrs$feature2 == 'Attack')
                               & corrs$toIgnore != 'Yes',][1:5],  row.names = FALSE)
   }, error = function(e){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }, warning = function(w){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }
)
```

```{r intent_01_feature_selection, cache=TRUE}
data1 <- data[,c("Land", "LoggedIn", "ProtocolType", "NumShells", "IsGuestLogin", "RootShell",  "Urgent", "Flag"
                 , "IsHostLogin", "NumFailedLogin", "Service", "Attack")]
```
```{r intent_01_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_01_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 100)
plot(output.forest, main="Intento 01")
```
```{r intent_01_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_01_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 02
<br>
El segundo intento trat√≥ de substituir las variables provistas en el modelo original por 
   - las que se han identificado como teniendo la mayor correlaci√≥n (coefficient >= 0.7 & p-value <= 0.05) con cada una de las variables respuestas que derivamos de los valores de "Attack" en la <a href="./01---Initial-Data-Discovery.html">fase inicial de exploraci√≥n de los datos</a>
   - pero tambi√©n aquellas que no estaban entre las m√°s fuertes correlaciones pero aparec√≠an entre las pocas identificadas para aquellas variables respuestas con pocas correlaciones, siendo as√≠ el caso de:
     + Duration - IsOtherAttack, coef.= -0.3077885, pvalue = 0
     + RootShell - IsU2Rattack, coef.= 0.365656, pvalue = 0
     + DiffSrvRate - IsProbeAttack, coef.= -0.5137125, pvalue = 0
     + DstHostDiffSrvRate - IsProbeAttack, coef.= -0.6604133, pvalue = 0

Este segundo intento mejor√≥ much√≠simo comparado con el primero pero sigue peor que los del modelo original con FALSE: 1935, TRUE: 67607. 

De este experimento, tambi√©n solo nos vamos al pr√≥ximo con la idea de conservar las variables que ten√≠an una correlaci√≥n fuerte (coefficient >= 0.7 & p-value <= 0.05).  

```{r getting_BestResponseVar_correlations, cache=TRUE, echo=FALSE}
tryCatch(
    expr = {
        exclude_fields <- c('Attack', 'AttackCat', 'IsAttack', 'IsDosAttack', 'IsProbeAttack', 'IsR2Lattack'
                            , 'IsU2Rattack', 'IsOtherAttack')
        knitr::kable(corrs[(corrs$feature1 %in% exclude_fields | corrs$feature2 %in% exclude_fields) 
                           & abs(corrs$coefficient) >= 0.7 & corrs$pvalue <= 0.05  
                           & corrs$toIgnore != 'Yes',][1:5],  row.names = FALSE)
   }, error = function(e){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }, warning = function(w){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }
)
```

```{r intent_02_feature_selection, cache=TRUE}
data1 <- data[,c("Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSameSrvRate", "DstHostSrvCount", "Count", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell",  "Duration", "Attack")]
```
```{r intent_02_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_02_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intento 02")
```
```{r intent_02_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_02_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 03
<br>
Este tercero intento fue poco diferente del anterior. En este solo  
   + substituimos las variables provistas en el modelo original por las que se han identificado como teniendo la mayor correlaci√≥n (coefficient >= 0.7 & p-value <= 0.05) con las variables respuestas derivadas desde "Attack"   
   + y con las que tienen buena relaci√≥n con la variable gen√©rica "IsAttack", aunque descinsideremos RerrorRate, SrvRerrorRate y DstHostSrvRerrorRate por tener una relaci√≥n muy fuerte.  
   
Este tercero intento nos da resultados semejantes al segundo pero, comparado con el original, sigue peor con FALSE: 1972, TRUE: 67570. 
  
De este experimento, tambi√©n solo nos vamos al pr√≥ximo con la idea de conservar las variables que tienen una correlaci√≥n fuerte.  

```{r intent_03_feature_selection, cache=TRUE}
data1 <- data[,c("Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSameSrvRate", "DstHostSrvCount", "Count"
                 , "DstHostRerrorRate", "RerrorRate", "SrvRerrorRate",  "DstHostSrvRerrorRate", "Attack")]
```
```{r intent_03_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_03_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intento 03")
```
```{r intent_03_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_03_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 04
<br>
En este cuarto intento, substituimos las variables provistas en el modelo original por¬† 
   + las que se han identificado como teniendo una correlaci√≥n con las variables respuestas derivadas desde "Attack" con un coeficiente >= 0.6 (menos DstHostSameSrvRate que tiene una correlaci√≥n fuerte con SameSrvRate),
   + manteniendo las del intento 02,
   + y DstHostRerrorRate del intento 03, pero removiendo RerrorRate, SrvRerrorRate, DstHostSrvRerrorRate porque tienen una correlaci√≥n entre ellas muy fuerte (coef. > 0.96) lo que puede acabar siendo redundante y al final no ayudar, entonces nos quedamos con aquella de las cuatro que tiene mayor coeficiente de correlaci√≥n con la variable respuesta "Attack". 
   
Esta vez, con este cuarto intento nos quedamos con resultados ligeramente mejores que los del original con FALSE: 1696, TRUE: 67846. 
       
```{r getting_Attack_GoodCorrelations, cache=TRUE, echo=FALSE}
tryCatch(
    expr = {knitr::kable(corrs[(corrs$feature1 == 'Attack' | corrs$feature2 == 'Attack')
                               & abs(corrs$coefficient) >= 0.6
                              & corrs$toIgnore != 'Yes',][1:5],  row.names = FALSE)
   }, error = function(e){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }, warning = function(w){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }
)
```
```{r intent_04_feature_selection, cache=TRUE}
data1 <- data[,c("Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSrvCount", "Count", "NumShells"
                 , "ProtocolType", "IsGuestLogin", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell",  "Duration"
                 , "DstHostRerrorRate", "Attack")]
```
```{r intent_04_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_04_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intento 04")
```
```{r intent_04_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_04_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 05
<br>
Para el quinto intento decidimos   
   + seguir con las variables utilizadas en el cuarto intento   
   + pero a√±adi√©ndoles una nueva, TotalBytes = SrcBytes + DstBytes.  
   
Esta vez, con este quinto intento se mejora significativamente los resultados comparados con los del modelo original con FALSE: 823, TRUE: 68719. 
  
  
```{r intent_05_feature_selection, cache=TRUE}
data <- data %>% mutate(TotalBytes = SrcBytes + DstBytes)
data1 <- data[,c("TotalBytes", "Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSrvCount", "Count", "NumShells"
                 , "ProtocolType", "IsGuestLogin", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell",  "Duration"
                 , "DstHostRerrorRate", "Attack")]
```
```{r intent_05_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_05_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intento 05")
```
```{r intent_05_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_05_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 06
<br>
En el sexto intento   
   + conservamos las variables del quinto intento   
   + pero esta vez a√±adi√©ndoles las variables que hab√≠an quedado fuera hasta ahora, pero tienen un coeficiente de correlaci√≥n >= 0.5 con las variables respuestas, siendo al final solo la adici√≥n de Urgent dado que las dem√°s ya estaban.     
   
Este sexto modelo sigue siendo significativamente mejor que el modelo original pero ligeramente peor que el quinto con FALSE: 906, TRUE: 68636. 

Considerado que el algoritmo ejecutado con randomforest() da raramente los mismos resultados entre iteraciones con un mismo conjunto de dados, tendremos cuidado de incluir o no la variable Urgent.   
   
```{r getting_GoodResponseVar_correlations, cache=TRUE, echo=FALSE}
tryCatch(
    expr = {
        exclude_fields <- c('Attack', 'AttackCat', 'IsAttack', 'IsDosAttack', 'IsProbeAttack', 'IsR2Lattack'
                            , 'IsU2Rattack', 'IsOtherAttack')
        knitr::kable(corrs[(corrs$feature1 %in% exclude_fields | corrs$feature2 %in% exclude_fields) 
                           & abs(corrs$coefficient) >= 0.5 & corrs$pvalue <= 0.05  
                           & corrs$toIgnore != 'Yes',][1:5],  row.names = FALSE)
   }, error = function(e){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }, warning = function(w){
        print(paste("Try executing the first R Markdown file: [01 - Initial Data Discovery.Rmd].\n", e), row.names=FALSE)
    }
)
```

```{r intent_06_feature_selection, cache=TRUE}
data1 <- data[,c("TotalBytes", "Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSrvCount", "Count", "NumShells"
                 , "Urgent", "ProtocolType", "IsGuestLogin", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell",  "Duration"
                 , "DstHostRerrorRate", "Attack")]
```
```{r intent_06_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_06_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intent 06")
```
```{r intent_06_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_06_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 07
<br>
Con el s√©ptimo intento   
   + conservamos las mismas variables que en el quinto intento   
   + pero, para intentar predecir mejor los tipos de ataques U2R, siendo este el tipo m√°s grave que deber√≠amos identificar, se les a√±adi√≥ una ‚Äúfeature‚Äù nueva en base a una combinaci√≥n de aquellas que tienen mayor correlaci√≥n con este tipo de ataques, NumCompromised, NumRoot, NumShells y NumFile.     

Este s√©ptimo modelo es equivalente al quinto con FALSE: 824, TRUE: 68718. 

Al final la nueva variable no parece haber agregado beneficio pero, como para el sexto experimento, considerado que el algoritmo ejecutado con randomforest() da raramente los mismos resultados entre iteraciones con un mismo conjunto de dados, tendremos cuidado de incluir o no la nueva variable U2RFeatures.   

```{r intent_07_feature_selection, cache=TRUE}
data$U2RFeatures <- as.integer(data$NumCompromised > 0 | data$NumRoot > 0 | data$NumShells > 0 | data$NumFile > 0)
data1 <- data[,c("U2RFeatures", "TotalBytes", "Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSrvCount"
                 , "Count", "NumShells", "ProtocolType", "IsGuestLogin", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell"
                 ,  "Duration", "DstHostRerrorRate", "Attack")]
```
```{r intent_07_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_07_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intento 07")
```
```{r intent_07_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_07_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Intento 08
<br>
Para el octavo intento,  
   + conservamos las mismas variables que en el quinto intento   
   + pero substituyendo algunas variables num√©ricas por otra que creemos que las resumen. 

La forma adoptada para sumarizar las variables num√©ricas fue de normalizarlas y despu√©s de sacar la m√©dia de los valores.
Para eso utilizamos la funci√≥n scale() que, seg√∫n chatGPT, 
   + calcula las puntuaciones z de las columnas num√©ricas especificadas,
   + resta la media de cada columna y la divide por la desviaci√≥n est√°ndar,
   + con lo que cada columna tiene una media de 0 y una desviaci√≥n est√°ndar de 1. 

"<i>Las puntuaciones Z, tambi√©n conocidas como puntuaciones est√°ndar, son una medida de cu√°ntas desviaciones est√°ndar hay entre una observaci√≥n o un punto de datos y la media de una distribuci√≥n. Una puntuaci√≥n z indica la posici√≥n relativa de un punto de datos individual en relaci√≥n con la media de la distribuci√≥n.
El c√°lculo de una puntuaci√≥n z consiste en restar la media de la distribuci√≥n del punto de datos individual y dividirla por la desviaci√≥n t√≠pica de la distribuci√≥n.</i>"
<br>

En este experimento hicimos m√°s de una tentativa pero las que realmente parecieron agregar algo fue la sumarizaci√≥n de las "RerrorRate". Al final, las variables realacionadas con RerrorRate aparecen entre las que tienen mayor correlaci√≥n entre ellas y tambi√©n con las variables respuestas. 
   + Las variables realacionadas con SerrorRate aparentaban tener una correlaci√≥n solo moderada con las variables respuestas.  
   + Las variables relacionadas con DiffSrvRate aparentaban tener una correlaci√≥n solo moderada con las variables respuestas, pero con IsProbeAttack aparec√≠a como teniendo una buena correlaci√≥n entonces tambi√©n la continuaremos derivando aunque no la selecionemos para el modelo general. 
   + Las variables relacionadas con SameSrvRate tampoco mejoraron el modelo, entonces preferimos guardarlas por separado. Estas ten√≠an una correlaci√≥n entre ellas con un coefficientes entre 0.83 y 0.94.
<br>

Finalmente este octavo modelo es el que mejores resultados dio con FALSE: 800, TRUE: 68742. 

Lo dejamos aqu√≠ y consideramos este modelo como el final.   

```{r intent_08_feature_selection, cache=TRUE}
normalized_Vars <- data.frame(scale(data[, c('DstHostRerrorRate', 'RerrorRate', 'SrvRerrorRate', 'DstHostSrvRerrorRate'
                                             , 'DiffSrvRate', 'DstHostDiffSrvRate'
                                             , 'SameSrvRate', 'SameSrvRate', 'DstHostSrvCount'
                                             , 'SerrorRate', 'DstHostSerrorRate', 'SrvSerrorRate', 'DstHostSrvSerrorRate')]))
data$SameSrvPct <- apply(normalized_Vars, 1, function(row){
    return(mean(row['SameSrvRate'], row['SameSrvRate'], row['DstHostSrvCount']))})
data$DiffSrvPct <- apply(normalized_Vars, 1, function(row){
    return(mean(row['DstHostDiffSrvRate'], row['DiffSrvRate']))})
data$RerrorsPct <- apply(normalized_Vars, 1, function(row){
    return(mean(row['DstHostRerrorRate'], row['RerrorRate'], row['SrvRerrorRate'], row['DstHostSrvRerrorRate']))})
data$SerrorsPct <- apply(normalized_Vars, 1, function(row){
    return(mean(row['DstHostSerrorRate'], row['SerrorRate'], row['SrvSerrorRate'], row['DstHostSrvSerrorRate']))})
data1 <- data[,c("TotalBytes", "Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSrvCount", "Count", "NumShells"
                 , "ProtocolType", "IsGuestLogin", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell",  "Duration"
                 , "RerrorsPct", "Attack")]
```
```{r intent_08_train_test, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
inTrain <- suppressWarnings(createDataPartition(y=data1$Attack,p=0.1, list=FALSE))
training <- data1[inTrain,]
testing <- data1[-inTrain,]
```
```{r intent_08_train_random_forest, echo=FALSE, cache=TRUE}
output.forest <- randomForest(Attack ~ ., data = training, ntree = 70)
plot(output.forest, main="Intento 08")
```
```{r intent_08_validations, echo=FALSE, cache=TRUE}
pred <- predict(output.forest,testing)
valid <- testing
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r intent_08_conf_matrix, echo=FALSE, cache=TRUE}
valid <- testing
cm <- confusionMatrix(pred, testing$Attack)
kableExtra::kable(cm$table)
```
<br>



### Evaluaci√≥n del modelo del experimiento 8 contra el fichero "Full"

El modelo ocho acert√≥ bastante bien con solo 1.15 % de errores, ya contra el fichero data_full no fue igual ya que observamos 6.96% de errores.

Por falta de m√°s tiempo ya lo dejamos aqu√≠ pero entendemos que habr√≠a mucho m√°s que mirar para intentar mejorar esta predicci√≥n, como por ejemplo: 
   * intentar jugar con el tama√±o de los ficheros de training y testing para optimizar los modelos derivados,
   * intentar utilizar algoritmos para clasificar las caracter√≠sticas y as√≠ identificar padrones que se nos habr√°n escapado,
   * intentar encontrar modelos para categor√≠a de ataque de forma a verificar si especializando los modelos y descartando los ataques ya encontrados se podr√≠a mejorar todav√≠a m√°s el desempe√±o,
   * conseguir utilizar otros algoritmos predictivos para compararlos con los resultados del Random Forest (intentamos sin √©xito usar el modelo neuronal - fit(), SVM -  svm() o CART - train()). 
<br><br>

```{r Final_Results, echo=FALSE, cache=TRUE}

cat(paste("         -------------------------------------\n         |    Modelo 8    | Contra data_full |\n"
          ,"--------|----------------|------------------|\n | FALSE |   800 |  1.15% |   21656 |  6.96% |\n"
          , "|  TRUE | 68742 | 98.85% |  289373 | 93.04% |\n --------|----------------|------------------|"), fill = TRUE)
```
```{r FullValidation_feature_selection, cache=TRUE}
data_full$Attack <- gsub("[.,]", "", data_full$Attack)
data_full <- data_full %>% mutate(TotalBytes = SrcBytes + DstBytes)
normalized_Vars <- data.frame(scale(data_full[, c('DstHostRerrorRate', 'RerrorRate', 'SrvRerrorRate'
                                                  , 'DstHostSrvRerrorRate', 'DiffSrvRate', 'DstHostDiffSrvRate'
                                                  , 'SameSrvRate', 'SameSrvRate', 'DstHostSrvCount', 'SerrorRate'
                                                  , 'DstHostSerrorRate', 'SrvSerrorRate', 'DstHostSrvSerrorRate')]))
data_full$RerrorsPct <- apply(normalized_Vars, 1, function(row){
    return(mean(row['DstHostRerrorRate'], row['RerrorRate'], row['SrvRerrorRate'], row['DstHostSrvRerrorRate']))})
data1 <- data_full[,c("TotalBytes", "Land", "LoggedIn", "Service", "Flag", "SameSrvRate", "DstHostSrvCount", "Count"
                      , "NumShells", "ProtocolType", "IsGuestLogin", "DiffSrvRate", "DstHostDiffSrvRate", "RootShell"
                      ,  "Duration", "RerrorsPct", "Attack")]
```
```{r FullValidation_Prediction, echo=FALSE, cache=TRUE}
data1$Attack <- as.factor(data1$Attack)
pred <- predict(output.forest,data1)
valid <- data1
valid$Attack <- as.character(valid$Attack)
valid$pred <- as.character(pred)
valid$match <- valid$Attack == valid$pred
table(valid$match)
table(valid[, c("match", "Attack")])
```
```{r FullValidation_conf_matrix, echo=FALSE, cache=TRUE}
valid <- data1
cm <- confusionMatrix(pred, data1$Attack)
kableExtra::kable(cm$table)
```

